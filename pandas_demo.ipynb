{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![Digital Futures](https://github.com/digital-futures-academy/DataScienceMasterResources/blob/main/Resources/datascience-notebook-header.png?raw=true)\n",
    "\n",
    "## Extracting Data using Python and Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is ***Pandas***?\n",
    "\n",
    "***Pandas*** is a Python library that provides data structures and data analysis tools. It is built on top of the [NumPy](https://numpy.org/) package, which means that a lot of the NumPy functions are also available in ***Pandas***.\n",
    "\n",
    "***Pandas*** is particularly useful for working with structured data, such as data stored in spreadsheets or databases, and it provides tools for reading and writing data between in-memory data structures and different file formats.\n",
    "\n",
    "The primary data structures in ***Pandas*** are the `Series` and `DataFrame` classes. A `Series` is essentially a column, and a `DataFrame` is a multi-dimensional table made up of a collection of `Series`.\n",
    "\n",
    "***Pandas*** provides many useful functions for working with data, such as grouping and aggregation functions, and it also provides plotting functions to create visualizations of the data.\n",
    "\n",
    "In this notebook, we will explore some of the basic functionality of ***Pandas***, including how to create `Series` and `DataFrame` objects, how to read and write data, and how to perform basic data manipulation tasks.\n",
    "\n",
    "For an overview of ***Pandas***, it is recommended that you view the [official Getting Started tutorial](https://pandas.pydata.org/docs/getting_started/intro_tutorials/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing ***Pandas***\n",
    "\n",
    "***Pandas*** is included in the Anaconda distribution, so if you have Anaconda installed, you should already have ***Pandas*** installed as well. If you don't have Anaconda installed, you can install ***Pandas*** using `pip`:\n",
    "\n",
    "If you are running this notebook in a Jupyter environment, you can run the following command to check if you have ***Pandas*** installed:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "print(pd.__version__)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # type: ignore\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an error, follow the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup Issues Scripts\n",
    "\n",
    "Make sure that you have the correct environment running - this includes the correct version of Python and all the necessary libraries.  If running in VSCode, you may need to create a virtual environment and activate it before running code cells notebook.\n",
    "\n",
    "If you are running this notebook after cloning and have not set up your environment to run shell commands, you will need to run the following commands in your terminal to set up the environment.\n",
    "\n",
    "> **NOTE:**  These commands need to be executed in the terminal.  \n",
    ">\n",
    "> Open a terminal at the root of your project before executing these commands\n",
    "> \n",
    "> Until your environment is set up, Jupyter Notebooks will not be able to run **shell** scripts.\n",
    "\n",
    "```sh\n",
    "# Create a virtual environment (add the command below)\n",
    "python3 -m venv .venv\n",
    "# Note: This command could also be python -m venv .venv \n",
    "# python3 and python are a symlink to the python version installed on your system\n",
    "\n",
    "# Activate the virtual environment \n",
    "source .venv/bin/activate\n",
    "\n",
    "# Install required package to execute shell commands from Jupyter Notebook\n",
    "pip install ipykernel pandas        ## OR \n",
    "pip install -r requirements.txt     ## IF there is already a requirements.txt file CONTAINING ipykenrnel in the project\n",
    "```\n",
    "\n",
    "`ipykernel` is a package that allows Jupyter Notebooks to run shell commands.\n",
    "\n",
    "Run the following command (again) to check if you have Pandas installed:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "print(pd.__version__)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # type: ignore\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Data from Files using Pandas\n",
    "\n",
    "### Reading Data from a Text File\n",
    "\n",
    "Pandas provides functions for reading data from a variety of file formats, including text files, CSV files, Excel files, and SQL databases.\n",
    "\n",
    "To read the data from a text file, you can load each line of the file into a `DataFrame` using the `read_csv()` function. The `read_csv()` function takes the path to the file as an argument and returns a `DataFrame` object.\n",
    "\n",
    "The text in this file are sentences.  Each sentence is on a new line.  We will read the file and create a `DataFrame` object with the sentences as the data.\n",
    "\n",
    "To do this:\n",
    "\n",
    "1. Make sure you have imported the Pandas library.\n",
    "2. Use the `read_csv()` function to read the data from the file.\n",
    "3. Display the `DataFrame` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data.txt', sep='\\r\\n', header=None, engine='python')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the `DataFrame` object has a single column with the sentences as the data.\n",
    "\n",
    "These are right aligned to a column named 0.  We can rename the column to something more meaningful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['Sentence']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Aside - Should we `try:except`?\n",
    "\n",
    "> YES, ABSOLUTELY!!! But we won't here to keep the demo simple!\n",
    ">\n",
    "> When reading data from a file, it is always a good idea to use a `try:except` block to catch any exceptions that may occur. This will prevent your program from crashing if there is an error while reading the file.  You can implement it in the same way as you would surround an `open` statement with a `try:except` block.\n",
    "\n",
    "```python\n",
    "# BETTER PRACTICE\n",
    "try:\n",
    "    # Read the data from the file\n",
    "    df = pd.read_csv('data/data.txt')\n",
    "    # Display the DataFrame\n",
    "    df\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "```\n",
    "\n",
    "> And you should anticipate the types of Exception or Errors that could occur and handle them accordingly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in from a CSV file\n",
    "\n",
    "The `read_csv()` function can also be used to read data from a CSV file.  The data in a CSV file is typically separated by commas, but the `read_csv()` function can handle other delimiters as well, as we have already seen!  (For clarity, the text file was seen as a single column of data, delimited by a newline character.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('data/data.csv')\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in from a TSV file\n",
    "\n",
    "A TSV file is a tab-separated values file, where the data is separated by tabs instead of commas.  The `read_table()` function should used to read data from a TSV file as it uses tab as a separator by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_table('data/data.tsv')\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also use the `read_csv()` function to read data from a TSV file, but we would need to specify the separator to be a tab using `delimiter='\\t'` in the arguments to the call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data from JSON\n",
    "\n",
    "Pandas can also read data from JSON files using the `read_json()` function.  JSON (JavaScript Object Notation) is a popular data format that is used for storing and exchanging data on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_json('data/data.json')\n",
    "\n",
    "df4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data from an Excel File\n",
    "\n",
    "Pandas can read data from Excel files using the `read_excel()` function.  Excel files can contain multiple sheets, so you can specify the sheet name or index to read the data from.\n",
    "\n",
    "Reading in data from an Excel file requires an additional library to be installed.  You can install the `openpyxl` library using `pip`:\n",
    "\n",
    "```sh\n",
    "pip install openpyxl\n",
    "```\n",
    "\n",
    "This will only work for .xlsx files.  If you have an older .xls file, you will need to install the `xlrd` library instead:\n",
    "\n",
    "```sh\n",
    "pip install xlrd\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install openpyxl and xlrd\n",
    "!pip install openpyxl xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.read_excel('data/data.xlsx', sheet_name='data')\n",
    "\n",
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data from a SQL Database\n",
    "\n",
    "Pandas can read data from SQL databases using the `read_sql()` wrapper function and its variants.  You need to establish a connection to the database using a library such as `sqlite3` or `psycopg2`, and then pass some SQL and the connection object to the `read_sql()` function.\n",
    "\n",
    "> We need a database to connect to, so let's create one that is in memory for demo purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an in-memory SQLite database\n",
    "\n",
    "We will use the `sqlite3` built-in library and also install the `sqlalchemy` library to create an in-memory SQLite database that will act as though its a \"real\" database and populate it with some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install sqlalchemy\n",
    "!pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary in-memory database using sqlite and sqlalchemy\n",
    "from sqlalchemy import create_engine, text # type: ignore\n",
    "\n",
    "# Create an in-memory database\n",
    "engine = create_engine('sqlite:///:memory:')\n",
    "\n",
    "# Connect to the database\n",
    "conn = engine.connect()\n",
    "\n",
    "# Create a table\n",
    "conn.execute(text('''\n",
    "CREATE TABLE users (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    name TEXT,\n",
    "    age INTEGER,\n",
    "    department TEXT,\n",
    "    location TEXT,\n",
    "    salary INTEGER\n",
    ")\n",
    "'''))\n",
    "\n",
    "\n",
    "# Insert multiple pieces of data into the table\n",
    "conn.execute(text('''\n",
    "INSERT INTO users (name, age, department, location, salary)\n",
    "VALUES\n",
    "('Alice', 25, 'Engineering', 'New York', 60000),\n",
    "('Bob', 30, 'HR', 'Los Angeles', 75000),\n",
    "('Charlie', 35, 'Marketing', 'Chicago', 55000),\n",
    "('Diana', 32, 'Finance', 'Boston', 78000),\n",
    "('Eve', 29, 'IT', 'Seattle', 77000)\n",
    "'''))\n",
    "\n",
    "# con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a connection to the database\n",
    "\n",
    "In the previous code cell, we have already connected to the in-memory SQLite database using a call to `connect` on the `engine` created by the call to the `sqlalchemy` library.  We have also created a `con` object to interact with the database.\n",
    "\n",
    "> If we were connecting to an external database using `psycopg2`, we would need to provide further details such as the database name, username, and password and use the connect function from this library:\n",
    "> \n",
    "> eg:\n",
    "> \n",
    "```python\n",
    "import psycopg2\n",
    "\n",
    "# Create the connection\n",
    "con = psycopg2.connect(\n",
    "    dbname='mydatabase',\n",
    "    user='myuser',\n",
    "    password='mypassword',\n",
    "    host='localhost'\n",
    "    port='5432'\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cur = con.cursor()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pandas to Read in Data from the Database\n",
    "\n",
    "Now we have a database we can query, we can use Pandas to read in data from the database using the `read_sql()` function which wraps around the `read_sql_table()` and `read_sql_query()` functions.\n",
    "\n",
    "There are different use cases for these functions:\n",
    "\n",
    "- `read_sql()` is a general-purpose wrapper function that can execute both SQL queries and SQL table reads\n",
    "  - *Most Common Parameters*:\n",
    "    - `sql`: The SQL query or table name to read from\n",
    "    - `con`: The connection object to the database\n",
    "    - `index_col`: The column to use as the index for the DataFrame (optional)\n",
    "    - `coerce_float`: Attempt to convert values to floats (optional)\n",
    "    - `params`: List of parameters to pass to the SQL query (optional)\n",
    "    - `parse_dates`: List of columns to parse as dates (optional)\n",
    "    - `columns`: List of columns to select from the SQL query (optional)\n",
    "    - `chunksize`: Number of rows to read at a time (optional)\n",
    "    - `dtype`: Dictionary of column names and types to use when reading the data (optional)\n",
    "    - `parse_dates`: List of columns to parse as dates (optional)\n",
    "\n",
    "- `read_sql_table` is a specific function for reading data from a SQL table\n",
    "  - *Most Common Parameters*:\n",
    "    - `table_name`: The name of the table to read from\n",
    "    - `con`: The connection object to the database\n",
    "    - `schema`: The schema to read the table from (optional)\n",
    "    - `index_col`: The column to use as the index for the DataFrame (optional)\n",
    "    - `coerce_float`: Attempt to convert values to floats (optional)\n",
    "    - `parse_dates`: List of columns to parse as dates (optional)\n",
    "    - `columns`: List of columns to select from the table (optional)\n",
    "    - `chunksize`: Number of rows to read at a time (optional)\n",
    "    - `dtype_backend`: Dictionary of column names and types to use when reading the data (optional)\n",
    "\n",
    "- `read_sql_query` is a specific function for reading data in from an SQL query\n",
    "  - *Most Common Parameters*:\n",
    "    - `sql`: The SQL query to read from\n",
    "    - `con`: The connection object to the database\n",
    "    - `index_col`: The column to use as the index for the DataFrame (optional)\n",
    "    - `coerce_float`: Attempt to convert values to floats (optional)\n",
    "    - `params`: List of parameters to pass to the SQL query (optional)\n",
    "    - `parse_dates`: List of columns to parse as dates (optional)\n",
    "    - `chunksize`: Number of rows to read at a time (optional)\n",
    "    - `dtype`: Dictionary of column names and types to use when reading the data (optional)\n",
    "    - `dtype_backend`: Back-end data type applied to the resultant DataFrame (still experimental)\n",
    "\n",
    "It may be best to construct our query as a string and pass it to the function.  This will allow us to reuse the query we are running and make it easier to debug if there are any issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1:  Read in Data from the Database\n",
    "\n",
    "#### Activity 1.1: Read the table directly using `read_sql()`\n",
    "\n",
    "1. In the cell below, get the ***users*** table in the database we created in memory using `read_sql()`.\n",
    "2. Use the `read_sql()` function to read in the data from the database.\n",
    "3. Display the `DataFrame` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = pd.read_sql('users', conn)\n",
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 1.2: Read the table directly using `read_sql_table()`\n",
    "\n",
    "1. In the cell below, get the ***users*** table in the database we created in memory using `read_sql_table()`.\n",
    "2. Use the `read_sql_table()` function to read in the data from the database.\n",
    "3. Display the `DataFrame` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = pd.read_sql_table('users', conn)\n",
    "df7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 1.3: Read the table via a query using `read_sql_query()`\n",
    "\n",
    "1. In the cell below, get the ***users*** table in the database we created in memory using `read_sql_query()`.\n",
    "   - This time we'll limit the rows to those who earn *more than 75000*.\n",
    "2. Define a variable called `more_than_75k` and set it to the SQL query string.\n",
    "3. Use the `read_sql_query()` function to read in the data from the database by passing the query string and the connection object.\n",
    "4. Display the `DataFrame` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'SELECT * FROM users WHERE salary > 75000'\n",
    "df8 = pd.read_sql_query(query, conn)\n",
    "df8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data in an HTML Table on a Web Page\n",
    "\n",
    "Commonly, you may wish to extract the data from a table on an HTML files.  Rather than going through the whole rigmarole of scraping the page, you can use the `read_html()` ***Pandas*** function to do this.\n",
    "\n",
    "The `read_html()` function takes a URL as an argument and returns a list of `DataFrame` objects, one for each table found on the page.  It does this by looking for the `<table>` tags in the HTML source code.  However, you do need to have a library installed to help parse the HTML - like `lxml` or `beautifulsoup4` (which also requires the `html5lib` package to be installed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install beautifulsoup4 and html5lib\n",
    "!pip install beautifulsoup4 html5lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the data folder there is a file called `data_in_tables.html` which contains some tables of data.\n",
    "> \n",
    "> The file can be served by installing the ***Live Preview*** extension in VSCode and right-clicking on the file and selecting \"Show Preview\".  \n",
    "> \n",
    "> This will open a browser window inside VSCode and display the web page.  \n",
    "> \n",
    "> This is server from a local webserver created on your machine that runs in the root of your project at `http://127.0.0.1` on port `3000`.  \n",
    "> \n",
    "> We can then navigate to the file by appending the file path to the URL, so in this case `/data/data_in_tables.html`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the tables in the HTML using the URL and BeautifulSoup to parse it\n",
    "df9 = pd.read_html(\"http://127.0.0.1:3000/data/data_in_tables.html\", flavor=\"bs4\")\n",
    "\n",
    "# Display the Data\n",
    "df9[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making API calls and Reading its Data to a DataFrame\n",
    "\n",
    "***Pandas*** can be used after we have made an API call to extract the data into different formats.  We can use the `requests` library to make the API call and then use ***Pandas*** to read the data into a DataFrame.\n",
    "\n",
    "You may expect that we will use the `read_json()` function within this operation but it becomes complicated as we need to convert the JSON data into a JSON string first.  However, if we make the request using the `requests` library and get the `response` in `JSON` format, we can simply pass this into a ***Pandas*** `DataFrame` object.\n",
    "\n",
    "First though, we need to install the `requests` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install requests\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can make a request to the API and convert the response into a `DataFrame`.\n",
    "\n",
    "> In this example, we will use some dummy data hosted at `https://jsonplaceholder.typicode.com/posts` which is a free API that provides dummy data for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the requests library\n",
    "import requests\n",
    "\n",
    "# Make a request to the API that returns data\n",
    "url = 'https://book-data-api.onrender.com/categories'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Convert the response body\n",
    "json_data = response.json()\n",
    "\n",
    "# Put the json_data into a DataFrame\n",
    "df10 = pd.DataFrame(json_data)\n",
    "\n",
    "# Display the Data\n",
    "df10[['category', 'number_of_books']].head(10)\n",
    "\n",
    "# Display the number of books in the category Fiction\n",
    "df10[df10['category']=='Fiction'][['category','number_of_books']]\n",
    "\n",
    "# Sort \n",
    "df10.sort_values('number_of_books', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you wanted to use the `read_json()` function, you would need to convert the JSON data into a JSON string first.  ***Pandas*** provides this functionality, but it is an unnecessary step!\n",
    "\n",
    "```python\n",
    "response = requests.get(url)\n",
    "\n",
    "json_data = response.json()\n",
    "\n",
    "df = pd.read_json(pd.io.json.dumps(json_data))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading a Parquet File into a Data Frame\n",
    "\n",
    "> #### What is a Parquet File?\n",
    ">\n",
    "> A Parquet file is a columnar storage file format that is optimized for reading and writing large datasets. It is designed to be highly efficient for analytical workloads, and it is widely used in the big data ecosystem.  They are binary files and so opening the `data.parquet` file in the `data` folder here will not display anything.\n",
    "\n",
    "We can use ***Pandas*** `read_parquet()` function to read in a Parquet file into a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11 = pd.read_parquet('data/data.parquet')\n",
    "df11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data in Other Formats\n",
    "\n",
    "Pandas have an extensive list of functions to read data from other formats.  You can find the full list of functions in the [Pandas documentation](https://pandas.pydata.org/docs/reference/io.html).\n",
    "\n",
    "The most notable not demonstrated are:\n",
    "\n",
    "- `read_table()` - Read general delimited file into DataFrame\n",
    "- `read_feather()` - Load a feather-format object from the file path - feather is a fast, lightweight, and easy-to-use binary file format for storing data frames\n",
    "- `read_gbq()` - Load data from Google BigQuery\n",
    "- `read_pickle()` - Load pickled pandas object (or any object) from file - a pickled pandas object is a serialized version of a pandas object that can be saved to disk and loaded back into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about Streamed Data?\n",
    "\n",
    "Data from streaming sources, such as Kafka, take a little more work to read into a `DataFrame`.  You would need to use a library such as `confluent_kafka` to read the data from the stream and then convert it into a `DataFrame`.  This will be covered in another session!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
